{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a4ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroquery.nasa_exoplanet_archive import NasaExoplanetArchive\n",
    "import pandas as pd\n",
    "\n",
    "# What to search for in when searching confirmed exoplanets\n",
    "where_con = (\n",
    "    \"koi_disposition = 'CONFIRMED' AND \" # Exoplanet must be confirmed (There are 4 Dispositions. 1- Confirmed 2- Candidate 3- Not Dispositioned 4- False Positive)\n",
    "    \"koi_depth > 500 AND \" # There must be a visible signal\n",
    "    \"koi_model_snr > 100 AND \" # The signal must be strong\n",
    "    \"koi_fpflag_nt = 0 AND \"\n",
    "    \"koi_fpflag_ss = 0 AND \"\n",
    "    \"koi_fpflag_co = 0 AND \"\n",
    "    \"koi_fpflag_ec = 0\"\n",
    ")\n",
    "\n",
    "where_fal = (\n",
    "    \"koi_disposition = 'FALSE POSITIVE' AND \" # Only search for false positives\n",
    "    \"koi_fpflag_nt = 1\" # Find false positives that are not transits\n",
    ")\n",
    "\n",
    "# Search the archive and return the exoplanets\n",
    "confirmed = NasaExoplanetArchive.query_criteria(\n",
    "    table=\"cumulative\",\n",
    "    select=\"kepid, koi_period, koi_duration, koi_time0bk, koi_depth, koi_model_snr, koi_disposition\",\n",
    "    where=where_con\n",
    ").to_pandas().dropna() # Drop record with empty parameters\n",
    "\n",
    "# Search the archive and return the false positives\n",
    "false_positives = NasaExoplanetArchive.query_criteria(\n",
    "    table=\"cumulative\",\n",
    "    select=\"kepid, koi_period, koi_duration, koi_time0bk, koi_depth, koi_model_snr, koi_disposition\",\n",
    "    where = where_fal\n",
    ").to_pandas().dropna()\n",
    "\n",
    "# Drop duplicate ids, keeping only the first. Only the strongest signal from any star will be in the dataset\n",
    "confirmed_unique = confirmed.sort_values(\"koi_disposition\").drop_duplicates(subset=\"kepid\", keep=\"first\")\n",
    "false_unique = false_positives.sort_values(\"koi_disposition\").drop_duplicates(subset=\"kepid\", keep=\"first\")\n",
    "\n",
    "# Print length for debugging purposes\n",
    "# print(len(confirmed_unique))\n",
    "# print(len(false_unique))\n",
    "\n",
    "# Remove overlapping ids, to avoid collision \n",
    "overlap_kepids = set(confirmed_unique['kepid']).intersection(set(false_unique['kepid']))\n",
    "confirmed_clean = confirmed_unique[~confirmed_unique['kepid'].isin(overlap_kepids)]\n",
    "false_clean = false_unique[~false_unique['kepid'].isin(overlap_kepids)]\n",
    "\n",
    "# Take a sample of 125 from the exoplanets and false positives\n",
    "confirmed_sample = confirmed_clean.sample(n=125, random_state=42)\n",
    "false_sample = false_clean.sample(n=125, random_state=42)\n",
    "\n",
    "# Merge and shuffle the dataset\n",
    "balanced_df = pd.concat([confirmed_sample, false_sample], ignore_index=True)\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save the dataset as a csv for later use\n",
    "balanced_df.to_csv(\"midSet.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e474f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import lightkurve as lk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Init\n",
    "kepid = 757450 # The id to test\n",
    "curve_length = 100 # How many points should the final curve be\n",
    "df = pd.read_csv(\"midSet.csv\") # Load the dataset\n",
    "row = df.loc[df[\"kepid\"] == kepid] # Define variables from the dataset\n",
    "period = row.iloc[0][\"koi_period\"]\n",
    "t0 = row.iloc[0][\"koi_time0bk\"]\n",
    "dur = row.iloc[0][\"koi_duration\"]\n",
    "fractional_duration = (dur / 24.0) / period\n",
    "print(row) # Log the info\n",
    "\n",
    "# test with KIC 757450 (Clear signal)\n",
    "lc = lk.search_lightcurve(\"KIC \" + str(kepid), mission=\"Kepler\", cadence = \"long\", limit = 6).download_all().stitch() # Download the light curves and \"Stitch\" them together\n",
    "lc.scatter() # Plot a scatter graph of the light curve\n",
    "lc = lc.remove_nans()\n",
    "\n",
    "# Flatten the light curve\n",
    "lc_flat = lc.flatten()\n",
    "lc_flat.scatter()\n",
    "\n",
    "# Fold the light curve on the period\n",
    "lc_fold = lc_flat.fold(period, epoch_time=t0)\n",
    "lc_fold.scatter()\n",
    "print(len(lc_fold))\n",
    "\n",
    "# How much data to include on either side of the curve\n",
    "buffer_factor = 2\n",
    "\n",
    "# Create a mask to isolate the curve\n",
    "window_half_width = fractional_duration * buffer_factor\n",
    "phase_mask = (lc_fold.phase > -(dur/24*0.5)*buffer_factor) & (lc_fold.phase < (dur/24*0.5)*buffer_factor)\n",
    "\n",
    "# Isolate the curve\n",
    "lc_zoom = lc_fold[phase_mask]\n",
    "lc_zoom.scatter()\n",
    "\n",
    "# Interpolate the curve\n",
    "time = lc_zoom.time.value\n",
    "flux = lc_zoom.flux.value\n",
    "t_min, t_max = time.min(), time.max()\n",
    "time_norm = (time - t_min) / (t_max - t_min)\n",
    "new_time_norm = np.linspace(0, 1, curve_length)\n",
    "new_flux = np.interp(new_time_norm, time_norm, flux)\n",
    "new_time = np.linspace(t_min, t_max, curve_length)\n",
    "int_lc = lk.LightCurve(time = new_time, flux = new_flux)\n",
    "int_lc.scatter()\n",
    "\n",
    "# Normalize the curve so that flux points are between 1 and 0\n",
    "min_flux = np.min(new_flux)\n",
    "max_flux = np.max(new_flux)\n",
    "scaled_flux = (new_flux - min_flux) / (max_flux - min_flux)\n",
    "\n",
    "lc_norm = lk.LightCurve(flux=scaled_flux, time=new_time)\n",
    "lc_norm.scatter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eccf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import lightkurve as lk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# WARNING, THIS TAKES A VERY LONG TIME, HOURS TO DOWNLOAD FROM THE ARCHIVE\n",
    "# Init\n",
    "df = pd.read_csv(\"midSet.csv\")\n",
    "kepids = df[\"kepid\"].unique().tolist()\n",
    "curve_length = 100\n",
    "buffer_factor = 2\n",
    "\n",
    "# Download all the curves\n",
    "for kepid in kepids:\n",
    "    if os.path.exists(f\"curvesBig/{kepid}.npz\"):\n",
    "        print(f\"File {kepid}.npz already exists. Skipping download.\")\n",
    "        continue  # Skip if the file already exists\n",
    "    print(\"Downloading: \" + str(kepid))\n",
    "\n",
    "    row = df.loc[df[\"kepid\"] == kepid]\n",
    "    period = row.iloc[0][\"koi_period\"]\n",
    "    t0 = row.iloc[0][\"koi_time0bk\"]\n",
    "    dur = row.iloc[0][\"koi_duration\"]\n",
    "    fractional_duration = (dur / 24.0) / period\n",
    "\n",
    "    # Retreive Light curve\n",
    "    lc = lk.search_lightcurve(\"KIC \" + str(kepid), mission=\"Kepler\", cadence = \"long\", limit = 10, author=\"Kepler\").download_all().stitch()\n",
    "    lc = lc.remove_nans()\n",
    "\n",
    "    # Flatten curve\n",
    "    lc_flat = lc.flatten()\n",
    "\n",
    "    # Fold curve\n",
    "    lc_fold = lc_flat.fold(period, epoch_time=t0)\n",
    "\n",
    "    # Define phase mask\n",
    "    window_half_width = fractional_duration * buffer_factor\n",
    "    phase_mask = (lc_fold.phase > -(dur/24*0.5)*buffer_factor) & (lc_fold.phase < (dur/24*0.5)*buffer_factor)\n",
    "\n",
    "    # Isolate curve\n",
    "    lc_zoom = lc_fold[phase_mask]\n",
    "\n",
    "    # Interpolate curve\n",
    "    time = lc_zoom.time.value\n",
    "    flux = lc_zoom.flux.value\n",
    "    t_min, t_max = time.min(), time.max()\n",
    "    time_norm = (time - t_min) / (t_max - t_min)\n",
    "    new_time_norm = np.linspace(0, 1, curve_length)\n",
    "    new_flux = np.interp(new_time_norm, time_norm, flux)\n",
    "    new_time = np.linspace(t_min, t_max, curve_length)\n",
    "\n",
    "    # Normalize Curve\n",
    "    min_flux = np.min(new_flux)\n",
    "    max_flux = np.max(new_flux)\n",
    "    scaled_flux = (new_flux - min_flux) / (max_flux - min_flux)\n",
    "\n",
    "    np.savez(f\"curvesBig/{kepid}.npz\", time=new_time, flux=scaled_flux) # Save the flux and time to the specified directory, the kepid is used as an identifier in the file name\n",
    "    print(\"Downloaded: \" + str(kepid))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670fc51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightkurve as lk\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"midSet.csv\")\n",
    "label_map = {\"CONFIRMED\": 1, \"FALSE POSITIVE\": 0} # Map the labels to 1 and 0 as this is what NN accepts\n",
    "\n",
    "# Define curve length for input shape\n",
    "curve_length = 100\n",
    "\n",
    "# Create empty lists for curves and labels\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Add curves and labels to lists\n",
    "print(\"Loading light curves...\")\n",
    "for _, row in df.iterrows():\n",
    "    kepid = row[\"kepid\"]\n",
    "    label = label_map.get(row[\"koi_disposition\"])\n",
    "\n",
    "    data = np.load(f\"curvesNorm/{kepid}.npz\")\n",
    "    time = data[\"time\"]\n",
    "    flux = data[\"flux\"]\n",
    "    \n",
    "    X.append(flux)\n",
    "    Y.append(label)\n",
    "    \n",
    "print(f\"Loaded {len(X)} light curves.\")\n",
    "\n",
    "# Make lists into numpy arrays to be used\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Test train split, 20%\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\n",
    "\n",
    "# The structure of the RNN\n",
    "print(\"Building model...\")\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(curve_length, 1)), # First LSTM layer, allows for return of sequences to improve accuracy\n",
    "    Dropout(0.3), # Dropout layer\n",
    "    LSTM(64), # Another LSTM layer\n",
    "    Dense(1, activation='sigmoid') # Sigmoid output layer for binary classifer\n",
    "])\n",
    "\n",
    "# Compile the model for binary cross entropy\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 50 epochs\n",
    "print(\"Training model...\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Log the training and testing accuracy\n",
    "scores = model.evaluate(X_train,Y_train)\n",
    "print(\"Training Accuracy: %.2f%%\\n\" % (scores[1]*100))\n",
    "\n",
    "scores = model.evaluate(X_test,Y_test)\n",
    "print(\"Testing Accuracy: %.2f%%\\n\" % (scores[1]*100))\n",
    "\n",
    "# Have the model predict the classes of the test data\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Apply sigmoid to predictions\n",
    "Y_predSig = [1 * (x[0]>=0.5) for x in Y_pred]\n",
    "\n",
    "# Plot the accuracy over the training period\n",
    "plt.plot(history.history[\"accuracy\"], label = \"Training Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label = \"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the loss over the training period\n",
    "plt.plot(history.history[\"loss\"], label = \"Training Loss\", c = \"green\")\n",
    "plt.plot(history.history[\"val_loss\"], label = \"Validation Loss\", c = \"Red\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Precision Score:\" + str(precision_score(Y_test, Y_predSig , average=\"macro\")))\n",
    "print(\"Recall Score:\" + str(recall_score(Y_test, Y_predSig , average=\"macro\")))\n",
    "print(\"F1 Score:\" + str(f1_score(Y_test, Y_predSig , average=\"macro\")))\n",
    "\n",
    "# Create a confusion matrix from the actuall classes and the predicted classes\n",
    "cm = confusion_matrix(Y_test, Y_predSig)\n",
    "\n",
    "# Confusion Metrics headers\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = [\"Exoplanet\",\"False Positive\"], \n",
    "                     columns = [\"Exoplanet\",\"False Positive\"])\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model, allows for future use\n",
    "model.save(\"exoV3.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import lightkurve as lk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Save images of all curves (Useful for dataset analysis)\n",
    "df = pd.read_csv(\"midSet.csv\")\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    kepid = row[\"kepid\"]\n",
    "    label = row[\"koi_disposition\"]\n",
    "\n",
    "    data = np.load(f\"curvesNorm/{kepid}.npz\")\n",
    "    time = data[\"time\"]\n",
    "    flux = data[\"flux\"]\n",
    "\n",
    "    lc = lk.LightCurve(time=time,flux=flux)\n",
    "    save = lc.scatter()\n",
    "\n",
    "    if(label == \"CONFIRMED\"):\n",
    "        save.figure.savefig(f\"curveImg/confirmed/{kepid}.png\")\n",
    "    elif(label == \"FALSE POSITIVE\"):\n",
    "        save.figure.savefig(f\"curveImg/falsePositive/{kepid}.png\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
